{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Modal Retrieval-Augmented Generation (RAG) System\n",
    "\n",
    "This project provides a multi-modal Retrieval-Augmented Generation (RAG) system that processes and analyzes text, tables, and media (images) for generating summaries and insights.\n",
    "\n",
    "### Generate API Key\n",
    "\n",
    "To use the ChatGoogleGenerativeAI model, you need an API key. You can generate your API key by following these steps:\n",
    "\n",
    "1. Go to [Google AI Studio API Key Generation](https://aistudio.google.com/app/apikey).\n",
    "2. Follow the instructions to generate your API key.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Multimodal-RAG-Meetup/blob/main/multimodel_rag.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install following dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pdfminer.six, unstructured pillow-heif pi_heif unstructured_inference pytesseract unstructured.pytesseract \"unstructured[all-docs]\" \n",
    "!pip install -U chromadb langchain langchain_huggingface langchain-google-genai langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from PIL import Image\n",
    "from unstructured.partition.pdf import partition_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_api_key = \"[GEMINI_API_KEY]\"\n",
    "document_name = \"[DOCUMENT_NAME_OR_PATH]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_content_with_metadata(file_path: str):\n",
    "    \"\"\"\n",
    "    Extract content, including images, tables, and text chunks from a PDF file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the PDF file to be processed.\n",
    "\n",
    "    Returns:\n",
    "        list[Element]: A list of Elements containing extracted elements like images, tables, and \n",
    "                    text, each represented in a structured format.\n",
    "    \"\"\"\n",
    "    return partition_pdf(\n",
    "        filename=file_path,\n",
    "        extract_images_in_pdf=True,\n",
    "        chunking_strategy=\"by_title\",\n",
    "        max_characters=4000,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        extract_image_block_types=[\"Image\", \"Table\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_pdf_elements_by_type(raw_pdf_elements: list):\n",
    "    \"\"\"\n",
    "    Categorize extracted PDF elements into text content.\n",
    "\n",
    "    Args:\n",
    "        raw_pdf_elements (list): List of unstructured documents elements extracted from the PDF.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of text elements extracted from the PDF.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for element in raw_pdf_elements:\n",
    "        if \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "            texts.append(str(element))\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract elements from the PDF\n",
    "raw_pdf_elements = extract_pdf_content_with_metadata(document_name)\n",
    "\n",
    "# Categorize elements into text content\n",
    "texts = categorize_pdf_elements_by_type(raw_pdf_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Google Generative AI model\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",     # Specify the model version\n",
    "    temperature=0,                # Set temperature for deterministic responses\n",
    "    max_output_tokens=2048,              # Set max tokens for each response\n",
    "    timeout=None,                 # No timeout for API calls\n",
    "    max_retries=2,                # Retry up to 2 times in case of failure\n",
    "    api_key=gemini_api_key        # Provide your API key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Generation Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate concise summaries for text elements\n",
    "def summarize_text_elements(text_elements: list):\n",
    "    \"\"\"\n",
    "    Generate concise summaries of text elements for efficient retrieval.\n",
    "\n",
    "    Args:\n",
    "        text_elements (list): List of strings representing the text elements to be summarized.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of concise summaries optimized for retrieval systems.\n",
    "    \"\"\"\n",
    "\n",
    "    # Updated prompt for summarization\n",
    "    summary_prompt = \"\"\"You are an assistant specializing in generating concise and accurate \n",
    "    summaries for retrieval purposes. The summary should capture the essential details of the \n",
    "    provided text, making it easily searchable and optimized for information retrieval. \n",
    "    Please provide a concise summary for the following element: {element}\"\"\"\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_template(summary_prompt)\n",
    "    \n",
    "    # Chain for processing and summarizing text elements\n",
    "    summary_chain = {\"element\": lambda x: x} | prompt_template | llm | StrOutputParser()\n",
    "\n",
    "    # Generate summaries with concurrency handling\n",
    "    summaries = summary_chain.batch(text_elements, {\"max_concurrency\": 5})\n",
    "\n",
    "    return summaries\n",
    "\n",
    "\n",
    "# Generate summaries for the provided text elements\n",
    "text_element_summaries = summarize_text_elements(text_elements=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_element_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_media_to_base64(media_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Encode media (image/table) to a base64 string.\n",
    "\n",
    "    Args:\n",
    "        media_path (str): The file path to the media.\n",
    "\n",
    "    Returns:\n",
    "        str: The base64-encoded string of the media.\n",
    "    \"\"\"\n",
    "    with open(media_path, \"rb\") as media_file:\n",
    "        return base64.b64encode(media_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def summarize_media(base64_media: str, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a summary for media (image/table) using the Google Generative AI model.\n",
    "\n",
    "    Args:\n",
    "        base64_media (str): Base64-encoded media string.\n",
    "        prompt (str): The prompt text to provide context for summarization.\n",
    "\n",
    "    Returns:\n",
    "        str: The summary of the media content.\n",
    "    \"\"\"\n",
    "    \n",
    "    msg = llm.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_media}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content # type: ignore\n",
    "\n",
    "\n",
    "def generate_media_summaries(media_dir: str):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for media (images/tables) in the given directory.\n",
    "\n",
    "    Args:\n",
    "        media_dir (str): Directory path containing .jpg or other media files.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], List[str]]: A tuple containing a list of base64-encoded media \n",
    "        and a list of their respective summaries.\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64-encoded media and summaries\n",
    "    base64_media_list = []\n",
    "    media_summaries = []\n",
    "\n",
    "    # Prompt for summarizing media (images/tables)\n",
    "    summary_prompt = \"\"\"You are an assistant tasked with summarizing images and tables for retrieval. \n",
    "    These summaries will be embedded and used to retrieve the raw media. \n",
    "    Provide concise summaries optimized for retrieval.\"\"\"\n",
    "\n",
    "    # Process each .jpg or media file in the directory\n",
    "    for media_file in sorted(os.listdir(media_dir)):\n",
    "        if media_file.endswith(\".jpg\"):  # You can extend this to include other media types\n",
    "            media_path = os.path.join(media_dir, media_file)\n",
    "            base64_media = encode_media_to_base64(media_path)\n",
    "            base64_media_list.append(base64_media)\n",
    "            media_summaries.append(summarize_media(base64_media, summary_prompt))\n",
    "\n",
    "    return base64_media_list, media_summaries\n",
    "\n",
    "\n",
    "# Generate media summaries and base64-encoded strings\n",
    "base64_media_list, media_summaries = generate_media_summaries(media_dir=\"figures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base64_media_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize HuggingFace embeddings using the specified model\n",
    "embedding_model_name = \"all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "# Now you can use `embeddings` to generate vector representations for your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_vector_retriever(\n",
    "    vectorstore: Chroma,\n",
    "    text_summaries: List[str],\n",
    "    raw_texts: List[str],\n",
    "    image_summaries: List[str],\n",
    "    raw_images: List[str]\n",
    ") -> MultiVectorRetriever:\n",
    "    \"\"\"\n",
    "    Create a retriever that indexes text and image summaries and returns raw text or image content.\n",
    "\n",
    "    Args:\n",
    "        vectorstore (Chroma): The vectorstore used to index document summaries.\n",
    "        text_summaries (List[str]): Summaries of text content to be indexed.\n",
    "        raw_texts (List[str]): Raw text documents corresponding to the text summaries.\n",
    "        image_summaries (List[str]): Summaries of image content to be indexed.\n",
    "        raw_images (List[str]): Base64-encoded image data corresponding to the image summaries.\n",
    "\n",
    "    Returns:\n",
    "        MultiVectorRetriever: A retriever that can fetch raw texts or images based on their summaries.\n",
    "    \"\"\"\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    def add_documents(\n",
    "        retriever: MultiVectorRetriever,\n",
    "        summaries: List[str],\n",
    "        contents: List[Union[str, bytes]]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Add documents to the vectorstore and docstore.\n",
    "\n",
    "        Args:\n",
    "            retriever (MultiVectorRetriever): The retriever instance.\n",
    "            summaries (List[str]): Document summaries to be added to the vectorstore.\n",
    "            contents (List[Union[str, bytes]]): Corresponding raw content (text or image) to be added to the docstore.\n",
    "        \"\"\"\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=summary, metadata={id_key: doc_ids[i]})\n",
    "            for i, summary in enumerate(summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, contents)))\n",
    "\n",
    "    # Add text and image summaries if available\n",
    "    if text_summaries and raw_texts:\n",
    "        add_documents(retriever, text_summaries, raw_texts)\n",
    "\n",
    "    if image_summaries and raw_images:\n",
    "        add_documents(retriever, image_summaries, raw_images)\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vectorstore with embeddings\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"mm_rag\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# Create the multi-vector retriever\n",
    "retriever_multi_vector = create_multi_vector_retriever(\n",
    "    vectorstore=vectorstore,\n",
    "    text_summaries=text_element_summaries,\n",
    "    raw_texts=texts,\n",
    "    image_summaries=media_summaries,\n",
    "    raw_images=base64_media_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_base64_encoded(data: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the string is a valid base64 encoded data.\n",
    "\n",
    "    Args:\n",
    "        data (str): String to be checked.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the string looks like base64 encoded data, otherwise False.\n",
    "    \"\"\"\n",
    "    return re.match(r\"^[A-Za-z0-9+/]+[=]{0,2}$\", data) is not None\n",
    "\n",
    "\n",
    "def is_media_base64(data: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the base64 data represents a media item.\n",
    "\n",
    "    Args:\n",
    "        data (str): Base64-encoded media data.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the data represents a media item, otherwise False.\n",
    "    \"\"\"\n",
    "    media_signatures = {\n",
    "        b\"\\xff\\xd8\\xff\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(data)[:8]  # Decode and get the first 8 bytes\n",
    "        return any(header.startswith(sig) for sig in media_signatures)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def resize_base64_media(base64_string: str, size: tuple = (128, 128)) -> str:\n",
    "    \"\"\"\n",
    "    Resize a base64-encoded media item.\n",
    "\n",
    "    Args:\n",
    "        base64_string (str): Base64-encoded media string.\n",
    "        size (tuple): New size for the media item.\n",
    "\n",
    "    Returns:\n",
    "        str: Base64-encoded string of the resized media item.\n",
    "    \"\"\"\n",
    "    media_data = base64.b64decode(base64_string)\n",
    "    media = Image.open(io.BytesIO(media_data))\n",
    "    resized_media = media.resize(size, Image.LANCZOS)\n",
    "\n",
    "    buffered = io.BytesIO()\n",
    "    resized_media.save(buffered, format=media.format)\n",
    "    \n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def split_media_and_texts(docs: List[str]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Split base64-encoded media and texts from a list of documents.\n",
    "\n",
    "    Args:\n",
    "        docs (List[str]): List of documents which might be base64-encoded media or texts.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[str]]: Dictionary with keys \"media\" and \"texts\" containing lists of base64 media and texts respectively.\n",
    "    \"\"\"\n",
    "    media_list = []\n",
    "    texts = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        if is_base64_encoded(doc) and is_media_base64(doc):\n",
    "            media_list.append(resize_base64_media(doc, size=(1300, 600)))\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    \n",
    "    return {\"media\": media_list, \"texts\": texts}\n",
    "\n",
    "\n",
    "def format_prompt(data: Dict[str, Union[List[str], str]]) -> List[HumanMessage]:\n",
    "    \"\"\"\n",
    "    Format data into a prompt for multi-modal analysis.\n",
    "\n",
    "    Args:\n",
    "        data (Dict[str, Union[List[str], str]]): Dictionary with \"context\" containing \"texts\" and \"media\", and a \"question\".\n",
    "\n",
    "    Returns:\n",
    "        List[HumanMessage]: List of messages formatted for the LLM.\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    if data[\"context\"][\"media\"]:\n",
    "        for media in data[\"context\"][\"media\"]:\n",
    "            media_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{media}\"},\n",
    "            }\n",
    "            messages.append(media_message)\n",
    "\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are an expert tasked with providing analysis and insights based on mixed media inputs.\\n\"\n",
    "            \"You will receive a combination of text, tables, and media items, including charts and graphs.\\n\"\n",
    "            \"Your goal is to provide a detailed analysis or answer based on the provided data.\\n\"\n",
    "            f\"User question: {data['question']}\\n\\n\"\n",
    "            \"Text and/or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    \n",
    "    return [HumanMessage(content=messages)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_modal_rag_chain(retriever) -> RunnableLambda:\n",
    "    \"\"\"\n",
    "    Create a multi-modal RAG chain for processing.\n",
    "\n",
    "    Args:\n",
    "        retriever (MultiVectorRetriever): The retriever instance.\n",
    "\n",
    "    Returns:\n",
    "        RunnableLambda: The RAG chain for processing.\n",
    "    \"\"\"\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_media_and_texts),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(format_prompt)\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create RAG Chain for Q & A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG chain\n",
    "chain_multimodal_rag = create_multi_modal_rag_chain(retriever_multi_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\n",
    "\n",
    "chain_multimodal_rag.invoke(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meetup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
