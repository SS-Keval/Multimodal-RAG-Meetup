{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Adaptive RAG system\n",
        "This project provides a simple implementation of Adaptive RAG pipeline. The pipeline classifies user queries and performs retrieval based on the classification. It also grades the retrieved documents and performs iterative retrieval if required.\n",
        "\n",
        "## Generate API Key\n",
        "\n",
        "### Gemini Flash\n",
        "To use the ChatGoogleGenerativeAI model, you need an API key. You can generate your API key by following these steps:\n",
        "\n",
        "1. Go to [Google AI Studio API Key Generation](https://aistudio.google.com/app/apikey).\n",
        "2. Follow the instructions to generate your API key.\n",
        "\n",
        "### Jina Embedding\n",
        "1. Go to [Jina Embeddings](https://jina.ai/embeddings/)\n",
        "2. From the API key and Billing section, copy the API key\n",
        "\n",
        "## Install Poppler and Tesseract\n",
        "For handling PDFs and unstructured data, you will need to install Poppler and Tesseract. Follow the installation instructions below:\n",
        "\n",
        "* Poppler: [Installation Instructions](https://pdf2image.readthedocs.io/en/latest/installation.html)\n",
        "* Tesseract: [Installation Instructions](https://tesseract-ocr.github.io/tessdoc/Installation.html)\n",
        "\n",
        "#### For Colab or Ubuntu\n",
        "Run the following commands in your terminal:\n",
        "\n",
        "```\n",
        "!sudo apt-get install poppler-utils\n",
        "!sudo apt install tesseract-ocr\n",
        "```\n",
        "\n",
        "This below installation section already contains the above commands. <br>\n",
        "**Note: During Colab installation, a message will pop-up \"Restart required\". Restart the runtime when prompted as this is necessary for installation.**"
      ],
      "metadata": {
        "id": "lFypKTHPG5MU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SS-Keval/Multimodal-RAG-Meetup/blob/main/adaptive_rag.ipynb)"
      ],
      "metadata": {
        "id": "QcxxdgP4OUH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "xXN5durh50Re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb langchain langchain_community langchain-chroma langchain-unstructured unstructured \"unstructured[all-docs]\" \"langchain-unstructured[local]\" langchain-google-genai"
      ],
      "metadata": {
        "id": "Q9BlpissBqIj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade nltk"
      ],
      "metadata": {
        "id": "Is6dFiU4A_kW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "3bPeaX0p6NJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils"
      ],
      "metadata": {
        "id": "9hsQmVAF83q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install tesseract-ocr"
      ],
      "metadata": {
        "id": "vEoMppVL9Yy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constants"
      ],
      "metadata": {
        "id": "U0Syx_On7v6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embeddings model\n",
        "EMBEDDING_MODEL_NAME = \"jina-embeddings-v2-base-en\"\n",
        "JINA_API_KEY = \"[ENTER API KEY]\"\n",
        "\n",
        "# LLM\n",
        "LLM_NAME = \"gemini-1.5-flash\"\n",
        "GOOGLE_API_KEY = \"[ENTER API KEY]\"\n",
        "\n",
        "# File path\n",
        "FILE_PATH = \"[ENTER FILE PATH]\""
      ],
      "metadata": {
        "id": "dav4A0ET7zIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing"
      ],
      "metadata": {
        "id": "WCp4vGpI54gZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "from unstructured.chunking.title import chunk_by_title"
      ],
      "metadata": {
        "id": "1_ob_jGP6paW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_pdf_elements = partition_pdf(filename=FILE_PATH)"
      ],
      "metadata": {
        "id": "f2yYRe1e7Pic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = chunk_by_title(\n",
        "  raw_pdf_elements,\n",
        "  max_characters=2000,\n",
        "  overlap=100,\n",
        "  multipage_sections=True,\n",
        "  new_after_n_chars=1000,\n",
        "  combine_text_under_n_chars=200\n",
        ")"
      ],
      "metadata": {
        "id": "5J-0nco2WAeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = []\n",
        "\n",
        "for chunk in chunks:\n",
        "  doc = Document(\n",
        "    page_content=chunk.text,\n",
        "    metadata={\n",
        "      \"page_no\": chunk.metadata.page_number\n",
        "    }\n",
        "  )\n",
        "  docs.append(doc)"
      ],
      "metadata": {
        "id": "udKJAg-HYVgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indexing"
      ],
      "metadata": {
        "id": "qyDvev_97ptV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.embeddings import JinaEmbeddings"
      ],
      "metadata": {
        "id": "FaRYWOVBBtAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chroma = chromadb.PersistentClient()"
      ],
      "metadata": {
        "id": "b8sU3c74BvGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ef = JinaEmbeddings(\n",
        "  jina_api_key=JINA_API_KEY, model_name=EMBEDDING_MODEL_NAME\n",
        ")"
      ],
      "metadata": {
        "id": "1UmeXcUEbu9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection = chroma.get_or_create_collection(\"data\")\n",
        "\n",
        "vectordb = Chroma(\n",
        "  client=chroma,\n",
        "  collection_name=\"data\",\n",
        "  embedding_function=ef\n",
        ")"
      ],
      "metadata": {
        "id": "C2Q0aZFRZz1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for start_index in range(0, len(docs), 32):\n",
        "  end_index = min(start_index + 32, len(docs))\n",
        "  chunks = docs[start_index:end_index]\n",
        "\n",
        "  vectordb.add_documents(chunks)\n",
        "  print(f\"Indexed {start_index+1}-{end_index} chunks\")\n"
      ],
      "metadata": {
        "id": "X4UXwV1UZzyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM"
      ],
      "metadata": {
        "id": "Em2zT_YV9hJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "bjmVkgy-cC-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_client = ChatGoogleGenerativeAI(\n",
        "  model=LLM_NAME,\n",
        "  google_api_key=GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "i5S5iKiC6_aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adaptive RAG"
      ],
      "metadata": {
        "id": "xZxnE8hwjOrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tools for query classification and document grading"
      ],
      "metadata": {
        "id": "7xlWK7xt9rNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "\n",
        "class QueryClassification(BaseModel):\n",
        "  \"\"\"Check whether the query is related to index\"\"\"\n",
        "\n",
        "  is_finance_related: bool = Field(description=\"True if the query is related to finance domain, else False\")\n",
        "\n",
        "\n",
        "class DocumentGrader(BaseModel):\n",
        "  \"\"\"Grade document either as relevant or irrelevant based on user query\"\"\"\n",
        "\n",
        "  is_relevant: bool = Field(description=\"Whether the document is useful in answering the query\")\n"
      ],
      "metadata": {
        "id": "xlHeHHGI0B_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _add_context_to_query(query: str, docs: list[Document]) -> str:\n",
        "  \"\"\"\n",
        "  Helper function to add context to retriever query\n",
        "\n",
        "  Args:\n",
        "    query (str): Input user query\n",
        "    docs (list[Document]): Relevant documents\n",
        "\n",
        "  Returns:\n",
        "    str: Query appended with the relevant documents\n",
        "  \"\"\"\n",
        "  context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "  query_with_context = query + \"\\n\\n\" + context\n",
        "\n",
        "  return query_with_context\n",
        "\n",
        "def classify_query(query: str) -> bool:\n",
        "  \"\"\"\n",
        "  Classify the input query as related or unrelated to the index\n",
        "\n",
        "  Args:\n",
        "    query (str): Input user query\n",
        "\n",
        "  Returns:\n",
        "    bool: True if the query is related to index else False\n",
        "  \"\"\"\n",
        "  classifier = llm_client.with_structured_output(QueryClassification)\n",
        "  out_class = classifier.invoke(query)\n",
        "\n",
        "  if out_class and out_class.is_finance_related:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def retriever(vectorstore: Chroma, query: str, docs: list[Document] | None = None) -> list[Document]:\n",
        "  \"\"\"\n",
        "  Fetch documents relevant to the query from the vectorstore\n",
        "  Relevant documents, if found in previous iteration, are appended to the query\n",
        "\n",
        "  Args:\n",
        "    vectorstore (Chroma): VectorDB instance\n",
        "    query (str): Input query\n",
        "    docs(list[Document] | None): Relevant documents found in previous iteration\n",
        "\n",
        "  Returns:\n",
        "    list[Document]: Documents semantically similar to user query\n",
        "  \"\"\"\n",
        "  retriever_query = query\n",
        "  if docs:\n",
        "    retriever_query = _add_context_to_query(query, docs)\n",
        "\n",
        "  docs = vectorstore.max_marginal_relevance_search(\n",
        "      query=retriever_query,\n",
        "      k=5,\n",
        "      fetch_k=20,\n",
        "  )\n",
        "\n",
        "  return docs\n",
        "\n",
        "def doc_grader(query: str, docs: list[Document]) -> tuple[list[bool], list]:\n",
        "  \"\"\"\n",
        "  Grades the documents fetched by retriever\n",
        "\n",
        "  Args:\n",
        "    query (str): Input query\n",
        "    docs (list[Document]): Documents to be graded\n",
        "\n",
        "  Returns:\n",
        "    list[bool]: Grade corresponding to each document\n",
        "    list: Relevant documents\n",
        "  \"\"\"\n",
        "  grades = []\n",
        "  relevant_docs = []\n",
        "\n",
        "  grader = llm_client.with_structured_output(DocumentGrader)\n",
        "  for doc in docs:\n",
        "    prompt = f\"\"\"Document:\n",
        "{doc.page_content}\n",
        "\n",
        "Query:\n",
        "{query}\n",
        "\"\"\"\n",
        "    result = grader.invoke(prompt)\n",
        "    grades.append(result.is_relevant)\n",
        "    if result.is_relevant:\n",
        "      relevant_docs.append(doc)\n",
        "\n",
        "  return grades, relevant_docs\n",
        "\n",
        "def generate_final_answer(query: str, relevant_docs: list[Document]) -> str:\n",
        "  \"\"\"\n",
        "  Final answer to user query using fetched documents\n",
        "\n",
        "  Args:\n",
        "    query (str): Input query\n",
        "    relevant_docs (list[Document]): Relevant documents fetched by the retriever\n",
        "\n",
        "  Returns:\n",
        "    str: Final answer\n",
        "  \"\"\"\n",
        "  context = \"\\n\\n\".join(doc.page_content for doc in relevant_docs)\n",
        "  messages = [\n",
        "      (\n",
        "          \"system\",\n",
        "          \"You are a helpful assistant that answers user queries. You will be provided with the context required to answer the user query. Answer queries using information from the provided context only.\"\n",
        "      ),\n",
        "      (\n",
        "          \"human\",\n",
        "          f\"\"\"Query:\n",
        "{query}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "      )\n",
        "  ]\n",
        "\n",
        "  final_answer = llm_client.invoke(messages)\n",
        "\n",
        "  return final_answer.content"
      ],
      "metadata": {
        "id": "Vm7XAsi7jQid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline(query: str, vectorstore: Chroma) -> str:\n",
        "  \"\"\"\n",
        "  Runs the adaptive rag pipeline for one or more retrievals\n",
        "\n",
        "  Args:\n",
        "    query (str): Input query\n",
        "    vectorstore (Chroma): VectorDB instance\n",
        "\n",
        "  Returns:\n",
        "    str: Response to the query\n",
        "  \"\"\"\n",
        "  docs = retriever(vectorstore, query)\n",
        "  grades, relevant_docs = doc_grader(query, docs)\n",
        "\n",
        "  print(f\"Retrieval grade - {sum(grades)}/{len(grades)}\")\n",
        "\n",
        "  if sum(grades)/len(grades) < 0.75:\n",
        "    print(\"Performing iterative retrieval\")\n",
        "    new_docs = retriever(vectorstore, query, relevant_docs)\n",
        "    relevant_docs.extend(new_docs)\n",
        "\n",
        "  return generate_final_answer(query, relevant_docs)"
      ],
      "metadata": {
        "id": "Vs3gkES2keIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adaptive_rag(query: str) -> str:\n",
        "  \"\"\"\n",
        "  Adaptive RAG pipeline to respond to user query by running 0 or more retrieval steps\n",
        "\n",
        "  Args:\n",
        "    query (str): Input query\n",
        "\n",
        "  Returns:\n",
        "    str: Response to the query\n",
        "  \"\"\"\n",
        "  query = query.strip()\n",
        "\n",
        "  if not query:\n",
        "    return\n",
        "\n",
        "  is_complex = classify_query(query)\n",
        "  if is_complex:\n",
        "    final_answer = run_pipeline(query=query, vectorstore=vectordb)\n",
        "    print(final_answer)\n",
        "  else:\n",
        "    messages = [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant that answers user queries. Be respectful and concise in your responses.\"\n",
        "        ),\n",
        "        (\n",
        "            \"human\",\n",
        "            query\n",
        "        )\n",
        "    ]\n",
        "    final_answer = llm_client.invoke(messages)\n",
        "    print(final_answer.content)"
      ],
      "metadata": {
        "id": "uMyIllUfGGiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adaptive_rag(\"What is the capital of India?\")"
      ],
      "metadata": {
        "id": "71g6jYAhHv56"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}